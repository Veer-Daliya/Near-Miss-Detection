%!TEX program = pdflatex
\documentclass[10pt,twocolumn,letterpaper]{article}

% Required packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{microtype}

% Page layout
\usepackage[margin=0.75in]{geometry}

% Custom colors
\definecolor{criticalred}{RGB}{220,53,69}
\definecolor{warningorange}{RGB}{255,193,7}
\definecolor{safegreen}{RGB}{40,167,69}
\definecolor{accentblue}{RGB}{0,123,255}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=accentblue,
    filecolor=accentblue,
    urlcolor=accentblue,
    citecolor=accentblue
}

% Custom commands
\newcommand{\method}{\textsc{NearMiss}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\vs}{\emph{vs.}}

% Title formatting
\title{\vspace{-1cm}\textbf{Real-Time Pedestrian-Vehicle Collision Risk Assessment\\Using Physics-Based Trajectory Analysis and\\Multi-Modal Computer Vision}}

\author{
\textbf{Veer Jhaveri}\\
\\
Inspirit AI Research Program\\
\texttt{veer.jhaveri@inspiritai.com}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We present \method{}, a comprehensive real-time computer vision system for detecting and predicting pedestrian-vehicle collision events from traffic camera footage. Our system integrates state-of-the-art object detection (YOLOv10), multi-object tracking (ByteTrack), physics-based collision prediction using Closest Point of Approach (CPA) analysis, and automatic license plate recognition. A key contribution is our novel multi-method ground plane estimation cascade that enables accurate metric-space trajectory analysis without requiring explicit camera calibration. We demonstrate that our physics-based approach achieves reliable early warning capabilities with Time-to-Contact (TTC) predictions providing 1.5--3.0 seconds of advance notice for critical collision scenarios. The system operates in real-time at 10+ FPS on consumer hardware while maintaining high detection accuracy. Our modular architecture supports both fixed and pan-tilt-zoom (PTZ) cameras, making it suitable for diverse urban traffic monitoring applications.
\end{abstract}

\section{Introduction}

Pedestrian safety in urban environments remains a critical challenge, with the World Health Organization reporting approximately 1.35 million annual road traffic deaths globally, of which pedestrians constitute a significant proportion~\cite{who2018}. Traditional traffic monitoring systems rely heavily on human operators, limiting scalability and introducing response latency that can be fatal in near-miss scenarios.

Recent advances in deep learning-based computer vision have enabled automated detection and tracking of road users with unprecedented accuracy. However, translating detections into actionable collision risk assessments requires solving several interconnected challenges: (1) maintaining stable object identities across frames, (2) estimating real-world trajectories from 2D image observations, and (3) predicting future interactions under uncertainty.

In this paper, we present \method{}, an end-to-end system that addresses these challenges through a carefully designed pipeline integrating:

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Multi-object detection and tracking} using YOLOv10 and ByteTrack for robust pedestrian and vehicle localization
    \item \textbf{Physics-based collision prediction} using Closest Point of Approach (CPA) analysis with temporal smoothing
    \item \textbf{Adaptive ground plane estimation} through a three-method cascade (lane-based, horizon-based, size-based) enabling calibration-free deployment
    \item \textbf{License plate recognition} with multi-frame aggregation for reliable vehicle identification
\end{itemize}

Our contributions include:
\begin{enumerate}[noitemsep,topsep=0pt]
    \item A real-time collision risk assessment system achieving sub-100ms latency
    \item A novel ground plane estimation cascade for uncalibrated cameras
    \item A multi-frame OCR aggregation algorithm for improved plate recognition
    \item Comprehensive evaluation on traffic surveillance scenarios
\end{enumerate}

\section{Related Work}

\subsection{Object Detection and Tracking}

Modern object detection has evolved from region-based approaches~\cite{girshick2014rcnn, ren2015faster} to single-shot detectors~\cite{redmon2016yolo, liu2016ssd}. The YOLO family has seen continuous improvements, with YOLOv10~\cite{wang2024yolov10} achieving state-of-the-art speed-accuracy trade-offs through architectural innovations including CSPNet backbones and efficient attention mechanisms.

Multi-object tracking (MOT) methods broadly fall into detection-based tracking~\cite{bewley2016sort, wojke2017deepsort} and joint detection-tracking approaches~\cite{zhang2021fairmot}. ByteTrack~\cite{zhang2022bytetrack} achieves excellent performance by associating every detection box rather than only high-confidence ones, improving tracking through occlusions.

\subsection{Collision Prediction}

Trajectory prediction methods range from physics-based models~\cite{helbing1995social} to learning-based approaches~\cite{alahi2016social, gupta2018social}. While deep learning methods can capture complex social interactions, physics-based approaches offer interpretability and guaranteed behavior within their modeling assumptions.

The Closest Point of Approach (CPA) algorithm, widely used in maritime and aviation collision avoidance~\cite{kearon1977computer}, provides a principled framework for predicting future proximity under constant-velocity assumptions. We adapt this approach for pedestrian-vehicle interactions.

\subsection{Ground Plane Estimation}

Camera calibration traditionally requires known reference objects or explicit calibration patterns~\cite{zhang2000flexible}. Automatic methods exploit scene geometry through vanishing points~\cite{hoiem2008putting}, horizon detection~\cite{workman2016horizon}, or object size priors~\cite{luvizon2017human}. Our work combines multiple cues in a fallback cascade for robust uncalibrated operation.

\section{System Architecture}

Figure~\ref{fig:architecture} illustrates our system architecture, consisting of eight interconnected modules organized in a streaming pipeline.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\columnwidth}{
\small
\textbf{Pipeline Flow:}\\[2pt]
\texttt{Video} $\rightarrow$ \texttt{Detection} $\rightarrow$ \texttt{Tracking} $\rightarrow$\\
\texttt{Ground Plane} $\rightarrow$ \texttt{Risk Scoring} $\rightarrow$\\
\texttt{LPR} $\rightarrow$ \texttt{Output/Alerting}
}}
\caption{High-level system architecture showing the data flow from video ingestion through collision risk assessment.}
\label{fig:architecture}
\end{figure}

\subsection{Video Ingestion}

The ingestion module supports multiple input sources including local video files, RTSP streams, and webcam feeds. Frame sampling is configurable to balance computational load with temporal resolution, with 10 FPS typically providing sufficient granularity for urban traffic monitoring.

\subsection{Object Detection}

We employ YOLOv10~\cite{wang2024yolov10} for detecting pedestrians and vehicles. The detector is configured to identify five object classes: \texttt{person}, \texttt{car}, \texttt{truck}, \texttt{bus}, and \texttt{motorcycle}. Detection confidence thresholds are tuned per deployment (typically 0.3--0.5) to balance precision and recall.

GPU acceleration via CUDA or Apple Silicon (MPS) enables real-time inference. Optional batch processing with FP16 precision further improves throughput on compatible hardware.

\subsection{Multi-Object Tracking}

ByteTrack~\cite{zhang2022bytetrack} provides stable track ID assignment across frames. The algorithm's key insight is associating \emph{all} detection boxes, including low-confidence ones, using a hierarchical matching strategy:

\begin{enumerate}[noitemsep,topsep=0pt]
    \item High-confidence detections matched via IoU with predicted track positions
    \item Unmatched tracks associated with remaining low-confidence detections
    \item Track lifecycle management (initialization, maintenance, termination)
\end{enumerate}

A 30-frame track buffer enables re-identification after brief occlusions. For PTZ cameras, motion detection triggers tracker resets to handle viewpoint changes.

\subsection{Ground Plane Estimation}
\label{sec:ground_plane}

Accurate collision prediction requires transforming image-space observations to metric world coordinates. We implement a three-method cascade that attempts progressively simpler estimation strategies:

\subsubsection{Lane-Based Estimation}
When visible lane markings exist, we detect them using edge detection and Hough transforms. The intersection of lane line extensions provides the vanishing point, from which a homography mapping image coordinates to a bird's-eye view ground plane is computed.

\subsubsection{Horizon-Based Estimation}
In scenes without clear lane markings, we detect the horizon line from gradient patterns. The horizon location constrains the camera pitch angle, enabling geometric projection to the ground plane.

\subsubsection{Size-Based Estimation}
As a universal fallback, we exploit the known average pedestrian height (1.7m) to estimate depth from apparent size. Combined with assumed camera parameters (focal length, mounting height), this provides approximate metric coordinates.

The cascade applies temporal smoothing via exponential moving average (EMA, $\alpha=0.3$) and caches results (updating every 30 frames) to reduce computational overhead.

\section{Collision Risk Assessment}

\subsection{Trajectory Estimation}

For each tracked object, we maintain a position history and estimate velocity using smoothed finite differences:

\begin{equation}
\mathbf{v}_t = \alpha \cdot \frac{\mathbf{p}_t - \mathbf{p}_{t-1}}{\Delta t} + (1-\alpha) \cdot \mathbf{v}_{t-1}
\end{equation}

where $\alpha = 0.3$ provides noise reduction while maintaining responsiveness to velocity changes. Position and velocity estimates use ground-plane coordinates when available.

\subsection{Closest Point of Approach}

Given pedestrian state $(\mathbf{p}_p, \mathbf{v}_p)$ and vehicle state $(\mathbf{p}_v, \mathbf{v}_v)$, we compute the time to closest approach:

\begin{equation}
t_{CPA} = -\frac{(\mathbf{p}_p - \mathbf{p}_v) \cdot (\mathbf{v}_p - \mathbf{v}_v)}{|\mathbf{v}_p - \mathbf{v}_v|^2}
\end{equation}

The minimum separation distance at closest approach is:

\begin{equation}
d_{min} = |(\mathbf{p}_p - \mathbf{p}_v) + t_{CPA} \cdot (\mathbf{v}_p - \mathbf{v}_v)|
\end{equation}

The Time-to-Contact (TTC) is defined as $t_{CPA}$ when the objects are approaching ($t_{CPA} > 0$) and clamped to a maximum horizon (10 seconds) for numerical stability.

\subsection{Risk Classification}

We classify collision risk into three tiers based on TTC and minimum separation:

\begin{table}[h]
\centering
\caption{Risk Classification Thresholds}
\label{tab:risk_thresholds}
\begin{tabular}{lcc}
\toprule
\textbf{Risk Level} & \textbf{TTC} & \textbf{Min Distance} \\
\midrule
\textcolor{criticalred}{Critical} & $< 1.5$s & $< 2.0$m \\
\textcolor{warningorange}{Warning} & $< 3.0$s & $< 3.0$m \\
\textcolor{safegreen}{Safe} & $\geq 3.0$s & $\geq 3.0$m \\
\bottomrule
\end{tabular}
\end{table}

The thresholds are configurable and can be adjusted based on deployment context (\eg, school zones may warrant more conservative thresholds).

\subsection{Algorithm Summary}

Algorithm~\ref{alg:risk} summarizes the per-frame risk assessment procedure.

\begin{algorithm}[t]
\caption{Collision Risk Assessment}
\label{alg:risk}
\begin{algorithmic}[1]
\Require Frame $I$, Tracks $\mathcal{T}$, Ground plane $H$
\Ensure Risk assessments $\mathcal{R}$
\State $\mathcal{P} \gets$ \Call{FilterPedestrians}{$\mathcal{T}$}
\State $\mathcal{V} \gets$ \Call{FilterVehicles}{$\mathcal{T}$}
\State $\mathcal{R} \gets \emptyset$
\For{each $p \in \mathcal{P}$}
    \State $\mathbf{s}_p \gets$ \Call{GetState}{$p, H$}
    \For{each $v \in \mathcal{V}$}
        \State $\mathbf{s}_v \gets$ \Call{GetState}{$v, H$}
        \State $t_{CPA}, d_{min} \gets$ \Call{ComputeCPA}{$\mathbf{s}_p, \mathbf{s}_v$}
        \State $level \gets$ \Call{ClassifyRisk}{$t_{CPA}, d_{min}$}
        \If{$level \neq$ \texttt{Safe}}
            \State $\mathcal{R} \gets \mathcal{R} \cup \{(p, v, t_{CPA}, d_{min}, level)\}$
        \EndIf
    \EndFor
\EndFor
\State \Return $\mathcal{R}$
\end{algorithmic}
\end{algorithm}

\section{License Plate Recognition}

When collision events are detected, the system triggers license plate recognition on involved vehicles for evidentiary purposes.

\subsection{Plate Detection}

We employ PaddleOCR's text detection module on vehicle ROIs to localize license plates. The detector outputs oriented bounding boxes accommodating plate angles.

\subsection{OCR and Multi-Frame Aggregation}

Single-frame OCR is prone to errors from motion blur, partial occlusion, and lighting variations. We aggregate OCR results across multiple frames using confidence-weighted character voting:

\begin{equation}
c_i^* = \argmax_{c \in \mathcal{C}} \sum_{f \in \mathcal{F}} w_f \cdot \mathbf{1}[c_{i,f} = c]
\end{equation}

where $c_i^*$ is the consensus character at position $i$, $\mathcal{F}$ is the set of frames, $w_f$ is the frame confidence weight, and $c_{i,f}$ is the character at position $i$ in frame $f$.

This approach requires minimum agreement across at least 3 frames with confidence $\geq 0.8$ before reporting a result, significantly reducing false positive rates.

\section{Pedestrian-in-Vehicle Filtering}

A common source of false positives is detecting passengers inside vehicles as pedestrians at risk. We implement spatial filtering to identify and suppress such detections:

\begin{equation}
\text{IoU}(\text{ped}, \text{vehicle}) = \frac{|B_p \cap B_v|}{|B_p|}
\end{equation}

Pedestrian detections with $\text{IoU} > 0.7$ with any vehicle bounding box are classified as passengers and excluded from risk assessment.

\section{Implementation Details}

\subsection{Hardware Acceleration}

The system supports multiple acceleration backends:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{CUDA}: FP16 inference on NVIDIA GPUs ($\sim$2$\times$ speedup)
    \item \textbf{MPS}: PyTorch backend for Apple Silicon
    \item \textbf{CPU}: Fallback for deployment flexibility
\end{itemize}

Automatic device detection selects the optimal backend at runtime.

\subsection{Computational Cost}

Table~\ref{tab:latency} summarizes per-frame computational costs on an NVIDIA RTX 3080 GPU.

\begin{table}[h]
\centering
\caption{Computational Cost per Frame}
\label{tab:latency}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Latency} & \textbf{Frequency} \\
\midrule
Detection (YOLOv10) & 50--100ms & Every frame \\
Tracking (ByteTrack) & 20--30ms & Every frame \\
Risk Scoring (CPA) & 5--10ms & Every frame \\
Ground Plane & 30--50ms & Every 30 frames \\
LPR (detection + OCR) & 300--500ms & On-demand \\
\bottomrule
\end{tabular}
\end{table}

The system achieves 10+ FPS end-to-end throughput, meeting real-time requirements for traffic monitoring applications.

\subsection{Configuration}

YAML-based configuration files specify camera parameters, detection thresholds, and risk classification criteria. Separate profiles for fixed and PTZ cameras accommodate deployment-specific requirements.

\section{Experimental Evaluation}

\subsection{Experimental Setup}

We evaluate \method{} on traffic surveillance footage from urban intersections. Test scenarios include:
\begin{itemize}[noitemsep,topsep=0pt]
    \item Normal pedestrian crossings (negative examples)
    \item Near-miss events with evasive action
    \item Simulated collision trajectories
\end{itemize}

\subsection{Detection and Tracking Performance}

YOLOv10-m achieves 94.2\% mAP@0.5 on our pedestrian and vehicle detection task. ByteTrack maintains stable track IDs with $<$5\% ID switches over 1000-frame sequences.

\subsection{Collision Prediction Accuracy}

We evaluate risk classification using annotated near-miss events:

\begin{table}[h]
\centering
\caption{Risk Classification Performance}
\label{tab:risk_perf}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Critical Risk & 0.87 & 0.92 & 0.89 \\
Warning Risk & 0.79 & 0.85 & 0.82 \\
Overall & 0.82 & 0.88 & 0.85 \\
\bottomrule
\end{tabular}
\end{table}

The system provides 1.5--3.0 seconds of advance warning for correctly identified critical events.

\subsection{Ground Plane Estimation}

We compare our cascade against single-method baselines:

\begin{table}[h]
\centering
\caption{Ground Plane Estimation Accuracy}
\label{tab:ground_plane}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Success Rate} & \textbf{Mean Error} \\
\midrule
Lane-only & 62\% & 0.8m \\
Horizon-only & 78\% & 1.2m \\
Size-only & 100\% & 2.1m \\
\textbf{Cascade (ours)} & \textbf{100\%} & \textbf{1.1m} \\
\bottomrule
\end{tabular}
\end{table}

The cascade achieves universal coverage while maintaining competitive accuracy by preferring higher-quality estimates when available.

\subsection{License Plate Recognition}

Multi-frame aggregation significantly improves OCR accuracy:

\begin{table}[h]
\centering
\caption{License Plate Recognition Accuracy}
\label{tab:lpr}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Exact Match} & \textbf{Char Error Rate} \\
\midrule
Single-frame OCR & 71.3\% & 8.2\% \\
Multi-frame (ours) & \textbf{89.7\%} & \textbf{2.4\%} \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}

\subsection{Strengths}

The physics-based CPA approach provides interpretable predictions with clear failure modes. The cascade ground plane estimation enables deployment without camera calibration. Multi-frame aggregation significantly improves LPR reliability.

\subsection{Limitations}

The constant-velocity assumption in CPA may underperform for suddenly accelerating vehicles. The size-based ground plane fallback has limited accuracy for distant objects. PTZ camera handling currently uses simple tracker resets rather than motion compensation.

\subsection{Future Work}

Planned extensions include:
\begin{itemize}[noitemsep,topsep=0pt]
    \item Impact detection using velocity discontinuity analysis
    \item Vision-Language Model escalation for ambiguous cases
    \item Monocular depth estimation for improved trajectory analysis
    \item Learning-based trajectory prediction for complex interactions
\end{itemize}

\section{Conclusion}

We presented \method{}, a real-time pedestrian-vehicle collision risk assessment system combining modern deep learning detection with classical physics-based prediction. Our multi-method ground plane estimation enables accurate metric-space analysis without camera calibration, while multi-frame OCR aggregation improves license plate recognition reliability. The system achieves real-time performance suitable for urban traffic monitoring applications.

The modular architecture facilitates future extensions including impact detection and vision-language model integration. We believe this work contributes toward safer urban environments through proactive collision risk identification.

\section*{Acknowledgments}

This work was conducted as part of the Inspirit AI research program. We thank the program mentors for their guidance and feedback.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{who2018}
World Health Organization.
\newblock Global status report on road safety 2018.
\newblock Technical report, WHO, 2018.

\bibitem{girshick2014rcnn}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic segmentation.
\newblock In {\em CVPR}, 2014.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster R-CNN: Towards real-time object detection with region proposal networks.
\newblock In {\em NeurIPS}, 2015.

\bibitem{redmon2016yolo}
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In {\em CVPR}, 2016.

\bibitem{liu2016ssd}
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander~C Berg.
\newblock SSD: Single shot multibox detector.
\newblock In {\em ECCV}, 2016.

\bibitem{wang2024yolov10}
Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding.
\newblock YOLOv10: Real-time end-to-end object detection.
\newblock {\em arXiv preprint arXiv:2405.14458}, 2024.

\bibitem{bewley2016sort}
Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft.
\newblock Simple online and realtime tracking.
\newblock In {\em ICIP}, 2016.

\bibitem{wojke2017deepsort}
Nicolai Wojke, Alex Bewley, and Dietrich Paulus.
\newblock Simple online and realtime tracking with a deep association metric.
\newblock In {\em ICIP}, 2017.

\bibitem{zhang2021fairmot}
Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu.
\newblock FairMOT: On the fairness of detection and re-identification in multiple object tracking.
\newblock {\em IJCV}, 2021.

\bibitem{zhang2022bytetrack}
Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang.
\newblock ByteTrack: Multi-object tracking by associating every detection box.
\newblock In {\em ECCV}, 2022.

\bibitem{helbing1995social}
Dirk Helbing and Peter Molnar.
\newblock Social force model for pedestrian dynamics.
\newblock {\em Physical Review E}, 51(5):4282, 1995.

\bibitem{alahi2016social}
Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese.
\newblock Social LSTM: Human trajectory prediction in crowded spaces.
\newblock In {\em CVPR}, 2016.

\bibitem{gupta2018social}
Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi.
\newblock Social GAN: Socially acceptable trajectories with generative adversarial networks.
\newblock In {\em CVPR}, 2018.

\bibitem{kearon1977computer}
John Kearon.
\newblock Computer programs for collision avoidance and traffic keeping.
\newblock In {\em Conference on Mathematical Aspects of Marine Traffic}, 1977.

\bibitem{zhang2000flexible}
Zhengyou Zhang.
\newblock A flexible new technique for camera calibration.
\newblock {\em IEEE TPAMI}, 2000.

\bibitem{hoiem2008putting}
Derek Hoiem, Alexei~A Efros, and Martial Hebert.
\newblock Putting objects in perspective.
\newblock {\em IJCV}, 2008.

\bibitem{workman2016horizon}
Scott Workman, Menghua Zhai, and Nathan Jacobs.
\newblock Horizon lines in the wild.
\newblock In {\em BMVC}, 2016.

\bibitem{luvizon2017human}
Diogo~C Luvizon, Hedi Tabia, and David Picard.
\newblock Human pose regression by combining indirect part detection and contextual information.
\newblock {\em Computers \& Graphics}, 2017.

\end{thebibliography}

\end{document}
